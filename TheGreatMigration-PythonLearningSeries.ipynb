{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='theTop'></a>\n",
    "## The Great Migration\n",
    "This project aims to understand the journey of a data engineer who wants to build their \"Hello, World!\" on Astra.\n",
    "\n",
    "#### The Data Engineer's \"Hello, World!\": \n",
    "Somewhere in the world üåé, a data engineer coming to Astra will want to perform The Great Migration üêò: moving their data from one cloud to Astra. For this example, we will see what this could be like for the dev moving data from GCP to Astra.\n",
    "\n",
    "This notebook has the following 6 üò± Sections:\n",
    "0. [Set-Up Stuff](#step0)\n",
    "\n",
    "Then...\n",
    "1. [Connect to GCP Big Query](#step1)\n",
    "2. [ETL BQ Schema to Astra Payload](#step2)\n",
    "3. [Create a table in Astra](#step3)\n",
    "4. [ETL BQ Data to Astra Payloads](#step4)\n",
    "5. [Insert the data in Astra](#step5)\n",
    "\n",
    "Confirmation Step: Successfully query the data from Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step0'></a>\n",
    "### 0. Set-Up\n",
    "1. Python packages\n",
    "2. Google Cloud SDK\n",
    "3. Astra Credentials\n",
    "4. [bonus] Connecting to BQ from Jupyter Notebooks\n",
    "\n",
    "###### [back to the top](#theTop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for Big Query\n",
    "from google.cloud import bigquery # the google cloud SDK\n",
    "\n",
    "# Packages for connecting to Astra\n",
    "import os # for accessing creds\n",
    "import requests # for making REST API requests\n",
    "import uuid # to create UUIDs for Astra connections\n",
    "import json # for converting json payloads to strings\n",
    "\n",
    "# Packages for ETL\n",
    "from copy import deepcopy # for schema ETL\n",
    "import unittest # for writing tests as we go\n",
    "import logging # for integrating into production codebase (instead of print())\n",
    "\n",
    "# Packages for notebook stuff\n",
    "from pprint import pprint # for pretty formatting\n",
    "import warnings # for supressing warnings in a notebook\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.2: Follow [these docs from Google](https://cloud.google.com/sdk/docs/quickstarts) on setting up your Cloud SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.3 Astra Credentials\n",
    "UUID = str(uuid.uuid1())\n",
    "USER = os.environ[\"ASTRA_USR\"]      # NEVER store your creds directly in your code!\n",
    "PASSWORD = os.environ[\"ASTRA_PSWD\"] # NEVER store your creds directly in your code!\n",
    "DB_ID = os.environ[\"ASTRA_DB\"]      # NEVER store your creds directly in your code!\n",
    "REGION = \"us-east1\"\n",
    "KYSPC = \"ds_metrics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.4 [bonus]: The Data Engineering team created [this video](https://datastax.zoom.us/rec/play/usAsdOH5pzk3TNGdtQSDV_F7W9XsK6-s1iQdrKcJyhvgVnkAYVfzZuNGMeN95iQWA_vuaqDra-Rd4eTq?continueMode=true&_x_zm_rtaid=arprVtbjSLKluh76R95SbQ.1594042537833.ccce8a5266f8f5fa88958ab4c2812e6d&_x_zm_rhtaid=167) on how to use Jupyter notebooks to connect to Big Query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### 1. Connect to GCP Big Query\n",
    "\n",
    "###### [back to the top](#theTop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Settings\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "PROJECT = \"aqueduct-production\"\n",
    "DATASET = \"ds_metrics\"\n",
    "TABLE = \"dse_new_customers_daily\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting schema\n",
    "def get_bq_schema(project, dataset, table):\n",
    "    \"\"\"\n",
    "        Return the schema for your BQ project as a df\n",
    "    \"\"\"\n",
    "    \n",
    "    query = f'SELECT * FROM {project}.{dataset}.INFORMATION_SCHEMA.COLUMNS'\n",
    "    df = client.query(query).to_dataframe()\n",
    "    \n",
    "    ## get the schema for only the table we want\n",
    "    bq_schema = df[df[\"table_name\"] == table]\n",
    "    bq_schema\n",
    "    return bq_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_catalog</th>\n",
       "      <th>table_schema</th>\n",
       "      <th>table_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>ordinal_position</th>\n",
       "      <th>is_nullable</th>\n",
       "      <th>data_type</th>\n",
       "      <th>is_generated</th>\n",
       "      <th>generation_expression</th>\n",
       "      <th>is_stored</th>\n",
       "      <th>is_hidden</th>\n",
       "      <th>is_updatable</th>\n",
       "      <th>is_system_defined</th>\n",
       "      <th>is_partitioning_column</th>\n",
       "      <th>clustering_ordinal_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>aqueduct-production</td>\n",
       "      <td>ds_metrics</td>\n",
       "      <td>dse_new_customers_daily</td>\n",
       "      <td>product</td>\n",
       "      <td>1</td>\n",
       "      <td>YES</td>\n",
       "      <td>STRING</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>aqueduct-production</td>\n",
       "      <td>ds_metrics</td>\n",
       "      <td>dse_new_customers_daily</td>\n",
       "      <td>download_date</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>aqueduct-production</td>\n",
       "      <td>ds_metrics</td>\n",
       "      <td>dse_new_customers_daily</td>\n",
       "      <td>count</td>\n",
       "      <td>3</td>\n",
       "      <td>YES</td>\n",
       "      <td>INT64</td>\n",
       "      <td>NEVER</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           table_catalog table_schema               table_name    column_name  \\\n",
       "112  aqueduct-production   ds_metrics  dse_new_customers_daily        product   \n",
       "113  aqueduct-production   ds_metrics  dse_new_customers_daily  download_date   \n",
       "114  aqueduct-production   ds_metrics  dse_new_customers_daily          count   \n",
       "\n",
       "     ordinal_position is_nullable data_type is_generated  \\\n",
       "112                 1         YES    STRING        NEVER   \n",
       "113                 2         YES      DATE        NEVER   \n",
       "114                 3         YES     INT64        NEVER   \n",
       "\n",
       "    generation_expression is_stored is_hidden is_updatable is_system_defined  \\\n",
       "112                  None      None        NO         None                NO   \n",
       "113                  None      None        NO         None                NO   \n",
       "114                  None      None        NO         None                NO   \n",
       "\n",
       "    is_partitioning_column clustering_ordinal_position  \n",
       "112                     NO                        None  \n",
       "113                     NO                        None  \n",
       "114                     NO                        None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_schema = get_bq_schema(PROJECT, DATASET, TABLE)\n",
    "bq_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### 2. ETL BQ Schema to Astra Payload\n",
    "\n",
    "###### [back to the top](#theTop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dict 1: Mapping Data Types from GCP to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['STRING', 'DATE', 'INT64'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_schema[\"data_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_bq_to_c = {\"STRING\": \"text\",\n",
    "                  \"DATE\": \"date\",\n",
    "                  \"INT64\": \"bigint\"\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dict 2: Table Definition Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of the required table structure for the schema\n",
    "table_definition = {\"name\": \"\",\n",
    "                    \"ifNotExists\": True,\n",
    "                    \"columnDefinitions\": [],\n",
    "                    \"primaryKey\": {\"partitionKey\": [],\n",
    "                                   \"clusteringKey\": []},\n",
    "                    \"tableOptions\": {\"defaultTimeToLive\":0}\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ETL BQ Table Schema to Astra Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_schema(table_name):\n",
    "    \"\"\"\n",
    "        Take a row from the BQ schema dataframe\n",
    "        Create the table definition, with its partition key\n",
    "        The partition key will be the table name (yes, this is bad)\n",
    "    \"\"\"\n",
    "    # create new table_definition\n",
    "    table_schema = {}\n",
    "    table_schema = deepcopy(table_definition)\n",
    "    table_schema[\"name\"] = table_name\n",
    "    \n",
    "    # set the table name as the partition key\n",
    "    partition_key = {}\n",
    "    partition_key = deepcopy(col_def)\n",
    "    partition_key[\"name\"] = table_name\n",
    "    partition_key[\"pos\"] = 0\n",
    "    partition_key[\"typeDefinition\"] = \"text\"\n",
    "    \n",
    "    # add a column to the table def for the part key\n",
    "    table_schema[\"columnDefinitions\"].append(partition_key)\n",
    "    \n",
    "    # set the table's part key to the table name\n",
    "    table_schema[\"primaryKey\"][\"partitionKey\"].append(table_name)\n",
    "    return table_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clustering_keys(table_name, astra_schema):\n",
    "    \"\"\"\n",
    "        Set the clustering columns for Astra Schema\n",
    "        According to their ordinal position of the schema from BQ\n",
    "    \"\"\"\n",
    "    for key, val in astra_schema.items():\n",
    "        ### set clustering columns\n",
    "        clusteringKey = []\n",
    "\n",
    "        # sort clustering keys based on ordinal pos\n",
    "        newlist = sorted(val[\"columnDefinitions\"], key=lambda k: k['pos']) \n",
    "\n",
    "        for col in newlist:\n",
    "            if col[\"pos\"] > 0: # (We don't want to repeat the partition key!)\n",
    "                clusteringKey.append(col[\"name\"])\n",
    "\n",
    "        val[\"primaryKey\"][\"clusteringKey\"] = clusteringKey\n",
    "        for col in val[\"columnDefinitions\"]:\n",
    "            col.pop(\"pos\", None)\n",
    "    return astra_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dict 3: Column Definition Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of the required column structure for the schema\n",
    "col_def = {\"name\":\"\",\n",
    "           \"typeDefinition\":\"\",\n",
    "           \"static\": False,\n",
    "           \"pos\": \"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ETL BQ Column Schema to Astra Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_col_schema(row):\n",
    "    \"\"\"\n",
    "        Take a row from the BQ schema dataframe\n",
    "        Create the table definition, with its partition key\n",
    "        The partition key will be the table name (yes, this is bad)\n",
    "    \"\"\"\n",
    "    new_col = {}\n",
    "    new_col = deepcopy(col_def)\n",
    "    new_col[\"name\"] = row[\"column_name\"]\n",
    "    new_col[\"typeDefinition\"] = schema_bq_to_c[row[\"data_type\"]]\n",
    "    new_col[\"pos\"] = row[\"ordinal_position\"]  # we will use this later\n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting It All Together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_bq_schema(table_name, bq_schema):\n",
    "    \"\"\"\n",
    "        Create a table definition dictionary\n",
    "        Set up the partition key\n",
    "        For all rows in the bq_schema dataframe\n",
    "            Create column definitions\n",
    "    \"\"\"\n",
    "    astra_schema = {}\n",
    "    astra_schema[table_name] = create_table_schema(table_name)\n",
    "    for index, row in bq_schema.iterrows():\n",
    "        # create new column definition for all other columns\n",
    "        new_col = create_col_schema(row)\n",
    "        # append to astra_tables[table_name][columnDefinitions]\n",
    "        astra_schema[table_name][\"columnDefinitions\"].append(new_col)\n",
    "    astra_schema = create_clustering_keys(table_name, astra_schema)\n",
    "    return astra_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'columnDefinitions': [{'name': 'dse_new_customers_daily',\n",
      "                        'static': False,\n",
      "                        'typeDefinition': 'text'},\n",
      "                       {'name': 'product',\n",
      "                        'static': False,\n",
      "                        'typeDefinition': 'text'},\n",
      "                       {'name': 'download_date',\n",
      "                        'static': False,\n",
      "                        'typeDefinition': 'date'},\n",
      "                       {'name': 'count',\n",
      "                        'static': False,\n",
      "                        'typeDefinition': 'bigint'}],\n",
      " 'ifNotExists': True,\n",
      " 'name': 'dse_new_customers_daily',\n",
      " 'primaryKey': {'clusteringKey': ['product', 'download_date', 'count'],\n",
      "                'partitionKey': ['dse_new_customers_daily']},\n",
      " 'tableOptions': {'defaultTimeToLive': 0}}\n"
     ]
    }
   ],
   "source": [
    "astra_schema = transform_bq_schema(TABLE, bq_schema)\n",
    "logging.debug(pprint(astra_schema[\"dse_new_customers_daily\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bonus Stuff: Write tests as you go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x109b23da0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## two tests to make sure we transformed BQ Schema to Astra Schema correctly\n",
    "class TestListElements(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.astra = list(astra_schema.keys())\n",
    "        self.bq = list(bq_schema[\"table_name\"].unique())\n",
    "\n",
    "    def test_count_eq(self):\n",
    "        self.assertCountEqual(self.astra, self.bq)\n",
    "\n",
    "    def test_list_eq(self):\n",
    "        self.assertListEqual(self.astra, self.bq)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=0, exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "### 3. Create Astra Table\n",
    "[Docs for payload structure are here](https://astra.readme.io/docs/creating-a-table-in-your-keyspace)\n",
    "\n",
    "\n",
    "###### [back to the top](#theTop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token():\n",
    "    \"\"\"\n",
    "        This helper function uses the auth REST API to get an access token\n",
    "        returns: an auth token; 30 minute expiration\n",
    "    \"\"\"\n",
    "    url = f\"https://{DB_ID}-{REGION}.apps.astra.datastax.com/api/rest/v1/auth\"\n",
    "    payload = {\"username\":USER,\n",
    "               \"password\":PASSWORD}\n",
    "    headers = {'accept': '*/*',\n",
    "               'content-type': 'application/json',\n",
    "               'x-cassandra-request-id': UUID}\n",
    "    # make auth request to Astra\n",
    "    r = requests.post(url, \n",
    "                     data=json.dumps(payload), \n",
    "                     headers=headers)\n",
    "    # raise any authentication errror\n",
    "    if r.status_code != 201:\n",
    "        raise Exception(r.text)   \n",
    "    # extract and return the auth token \n",
    "    data = json.loads(r.text)\n",
    "    return data[\"authToken\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(table_def):\n",
    "    \"\"\"\n",
    "        Post the table_def dictionary to the Astra REST API\n",
    "        Return the response for downstream logging\n",
    "    \"\"\"\n",
    "    url = f'https://{DB_ID}-{REGION}.apps.astra.datastax.com/api/rest/v1/keyspaces/{KYSPC}/tables'\n",
    "    headers = {'accept': '*/*',\n",
    "               'content-type': 'application/json',\n",
    "               'x-cassandra-request-id': UUID,\n",
    "               'x-cassandra-token': token} \n",
    "    r = requests.post(url, \n",
    "                 data=json.dumps(table_def),\n",
    "                 headers=headers)\n",
    "    # raise any authentication errror\n",
    "    if r.status_code != 201:\n",
    "        logging.debug(r.status_code)\n",
    "        raise Exception(r.text) \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): 8aa4160b-0096-4061-8cd9-e17ffa74b595-us-east1.apps.astra.datastax.com:443\n",
      "DEBUG:urllib3.connectionpool:https://8aa4160b-0096-4061-8cd9-e17ffa74b595-us-east1.apps.astra.datastax.com:443 \"POST /api/rest/v1/auth HTTP/1.1\" 201 52\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): 8aa4160b-0096-4061-8cd9-e17ffa74b595-us-east1.apps.astra.datastax.com:443\n",
      "DEBUG:urllib3.connectionpool:https://8aa4160b-0096-4061-8cd9-e17ffa74b595-us-east1.apps.astra.datastax.com:443 \"POST /api/rest/v1/keyspaces/ds_metrics/tables HTTP/1.1\" 201 16\n",
      "DEBUG:root:{\"success\":true}\n"
     ]
    }
   ],
   "source": [
    "token = get_token()\n",
    "result = create_table(astra_schema[TABLE])\n",
    "logging.debug(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "\n",
    "### 4. ETL Data from Big Query to Astra Payloads\n",
    "\n",
    "###### [back to the top](#theTop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Data from Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get data from BQ\n",
    "def get_data_from_BQ(project, dataset, table):\n",
    "    \"\"\"\n",
    "        Helper function that pulls all data from BQ for table\n",
    "        returns: the data as a dataframe\n",
    "    \"\"\"\n",
    "    query = f'SELECT * FROM {project}.{dataset}.{table}'\n",
    "    df = client.query(query).to_dataframe()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: bigquery.googleapis.com\n",
      "DEBUG:urllib3.connectionpool:https://bigquery.googleapis.com:443 \"POST /bigquery/v2/projects/aqueduct-production/jobs HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/aqueduct-production/queries/f9a62baf-1837-4df4-98b2-275282f096c6?maxResults=0&location=US HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/aqueduct-production/jobs/f9a62baf-1837-4df4-98b2-275282f096c6?location=US HTTP/1.1\" 200 None\n",
      "DEBUG:google.cloud.bigquery.table:Started reading table 'aqueduct-production._a645f1a52b3512d886eab733a954fd298a614be6.anon26d6fa33_2a8e_4e9e_8e58_dddc5f445e67' with tabledata.list.\n",
      "DEBUG:urllib3.connectionpool:https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/aqueduct-production/datasets/_a645f1a52b3512d886eab733a954fd298a614be6/tables/anon26d6fa33_2a8e_4e9e_8e58_dddc5f445e67/data HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>download_date</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DataStax Enterprise-VER_UNK</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DataStax Enterprise-VER_UNK</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DataStax Enterprise-6.8</td>\n",
       "      <td>2020-07-16</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DataStax Enterprise-6.8</td>\n",
       "      <td>2020-07-15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DataStax Enterprise-6.8</td>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>DataStax Enterprise-1.0</td>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>DataStax Enterprise-1.0</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>DataStax Enterprise-1.0</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>DataStax Enterprise-1.0</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>DataStax Enterprise-1.0</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1970 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          product download_date  count\n",
       "0     DataStax Enterprise-VER_UNK    2020-02-05      1\n",
       "1     DataStax Enterprise-VER_UNK    2020-01-28      1\n",
       "2         DataStax Enterprise-6.8    2020-07-16     28\n",
       "3         DataStax Enterprise-6.8    2020-07-15     18\n",
       "4         DataStax Enterprise-6.8    2020-07-14     38\n",
       "...                           ...           ...    ...\n",
       "1965      DataStax Enterprise-1.0    2020-01-17      1\n",
       "1966      DataStax Enterprise-1.0    2020-01-16      3\n",
       "1967      DataStax Enterprise-1.0    2020-01-05      2\n",
       "1968      DataStax Enterprise-1.0    2020-01-03      1\n",
       "1969      DataStax Enterprise-1.0    2020-01-01      5\n",
       "\n",
       "[1970 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data_from_BQ(PROJECT, DATASET, TABLE) # data frame of data from BQ\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETL the Data to Payloads for Astra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL from BQ dataframe to Astra payloads of {name: foo, value: bar} \n",
    "def transform_data(table, keys, data):\n",
    "    \"\"\"\n",
    "        keys: the column names\n",
    "        data: dataframe of BQ data\n",
    "        returns: a list of payloads {\"name\":\"name0\",\"value\":\"value0\"} \n",
    "    \"\"\"\n",
    "    inserts = []\n",
    "    for index, row in data.iterrows():\n",
    "        all_data = []\n",
    "        # the partition key\n",
    "        data_dict = {}\n",
    "        data_dict[\"name\"] = table\n",
    "        data_dict[\"value\"] = table\n",
    "        all_data.append(data_dict)\n",
    "        for key in keys:\n",
    "            data_dict = {}\n",
    "            data_dict[\"name\"] = str(key).lower()\n",
    "            data_dict[\"value\"] = str(row[key]).lower()\n",
    "            all_data.append(data_dict)\n",
    "        inserts.append(all_data)\n",
    "    return inserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'dse_new_customers_daily', 'value': 'dse_new_customers_daily'},\n",
      " {'name': 'product', 'value': 'datastax enterprise-ver_unk'},\n",
      " {'name': 'download_date', 'value': '2020-02-05'},\n",
      " {'name': 'count', 'value': '1'}]\n"
     ]
    }
   ],
   "source": [
    "inserts = transform_data(TABLE, list(data.columns), data)\n",
    "logging.debug(pprint(inserts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BONUS! ETL Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:1970\n"
     ]
    }
   ],
   "source": [
    "logging.debug(len(data.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x11eb97be0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## two tests to make sure we transformed BQ Schema to Astra Schema correctly\n",
    "class TestListElements(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.astra = inserts\n",
    "        self.bq = data.values.tolist()\n",
    "\n",
    "    def test_count_eq(self):\n",
    "        self.assertEqual(len(self.astra), len(self.bq))\n",
    "\n",
    "unittest.main(argv=[''], verbosity=0, exit=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "### 5. Post data Payloads to Astra\n",
    "\n",
    "###### [back to the top](#theTop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_data_to_astra(table, payload):\n",
    "    \"\"\"\n",
    "        Post the payload to insert into the table\n",
    "    \"\"\"\n",
    "    url = f\"https://{DB_ID}-{REGION}.apps.astra.datastax.com/api/rest/v1/keyspaces/{KYSPC}/tables/{table}/rows\"\n",
    "    headers = {'accept': 'application/json',\n",
    "               'content-type': 'application/json',\n",
    "               'x-cassandra-request-id': UUID,\n",
    "               'x-cassandra-token': token}\n",
    "    r = requests.post(url, \n",
    "                      data=json.dumps(payload),\n",
    "                      headers=headers)\n",
    "    # raise any errrors\n",
    "    if r.status_code != 201:\n",
    "        logging.debug(r.status_code)\n",
    "        raise Exception(r.text)\n",
    "    return(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(table, inserts):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    num_records = 0\n",
    "    num_failed = 0\n",
    "    logging.info(f'Expecting to insert: {len(inserts)} total rows into table {table}')\n",
    "    for data in inserts:\n",
    "        total = num_records + num_failed\n",
    "        if(total % 10 == 0) and (total > 0):\n",
    "            logging.info(f'\\t Processed {total} rows ...')\n",
    "        payload = {\"columns\": data}\n",
    "        response = post_data_to_astra(table.lower(), payload)\n",
    "        if response.status_code != 201:\n",
    "            num_failed += 1\n",
    "        else:\n",
    "            num_records += 1\n",
    "    logging.info(\"Done!\")\n",
    "    logging.info(f'\\t Inserted: {num_records} total rows')\n",
    "    logging.info(f'\\t Failed to insert: {num_failed} total rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Expecting to insert: 1970 total rows into table dse_new_customers_daily\n",
      "INFO:root:\t Processed 10 rows ...\n",
      "INFO:root:\t Processed 20 rows ...\n",
      "INFO:root:\t Processed 30 rows ...\n",
      "INFO:root:\t Processed 40 rows ...\n",
      "INFO:root:\t Processed 50 rows ...\n",
      "INFO:root:\t Processed 60 rows ...\n",
      "INFO:root:\t Processed 70 rows ...\n",
      "INFO:root:\t Processed 80 rows ...\n",
      "INFO:root:\t Processed 90 rows ...\n",
      "INFO:root:\t Processed 100 rows ...\n",
      "INFO:root:\t Processed 110 rows ...\n",
      "INFO:root:\t Processed 120 rows ...\n",
      "INFO:root:\t Processed 130 rows ...\n",
      "INFO:root:\t Processed 140 rows ...\n",
      "INFO:root:\t Processed 150 rows ...\n",
      "INFO:root:\t Processed 160 rows ...\n",
      "INFO:root:\t Processed 170 rows ...\n",
      "INFO:root:\t Processed 180 rows ...\n",
      "INFO:root:\t Processed 190 rows ...\n",
      "INFO:root:\t Processed 200 rows ...\n",
      "INFO:root:\t Processed 210 rows ...\n",
      "INFO:root:\t Processed 220 rows ...\n",
      "INFO:root:\t Processed 230 rows ...\n",
      "INFO:root:\t Processed 240 rows ...\n",
      "INFO:root:\t Processed 250 rows ...\n",
      "INFO:root:\t Processed 260 rows ...\n",
      "INFO:root:\t Processed 270 rows ...\n",
      "INFO:root:\t Processed 280 rows ...\n",
      "INFO:root:\t Processed 290 rows ...\n",
      "INFO:root:\t Processed 300 rows ...\n",
      "INFO:root:\t Processed 310 rows ...\n",
      "INFO:root:\t Processed 320 rows ...\n",
      "INFO:root:\t Processed 330 rows ...\n",
      "INFO:root:\t Processed 340 rows ...\n",
      "INFO:root:\t Processed 350 rows ...\n",
      "INFO:root:\t Processed 360 rows ...\n",
      "INFO:root:\t Processed 370 rows ...\n",
      "INFO:root:\t Processed 380 rows ...\n",
      "INFO:root:\t Processed 390 rows ...\n",
      "INFO:root:\t Processed 400 rows ...\n",
      "INFO:root:\t Processed 410 rows ...\n",
      "INFO:root:\t Processed 420 rows ...\n",
      "INFO:root:\t Processed 430 rows ...\n",
      "INFO:root:\t Processed 440 rows ...\n",
      "INFO:root:\t Processed 450 rows ...\n",
      "INFO:root:\t Processed 460 rows ...\n",
      "INFO:root:\t Processed 470 rows ...\n",
      "INFO:root:\t Processed 480 rows ...\n",
      "INFO:root:\t Processed 490 rows ...\n",
      "INFO:root:\t Processed 500 rows ...\n",
      "INFO:root:\t Processed 510 rows ...\n",
      "INFO:root:\t Processed 520 rows ...\n",
      "INFO:root:\t Processed 530 rows ...\n",
      "INFO:root:\t Processed 540 rows ...\n",
      "INFO:root:\t Processed 550 rows ...\n",
      "INFO:root:\t Processed 560 rows ...\n",
      "INFO:root:\t Processed 570 rows ...\n",
      "INFO:root:\t Processed 580 rows ...\n",
      "INFO:root:\t Processed 590 rows ...\n",
      "INFO:root:\t Processed 600 rows ...\n",
      "INFO:root:\t Processed 610 rows ...\n",
      "INFO:root:\t Processed 620 rows ...\n",
      "INFO:root:\t Processed 630 rows ...\n",
      "INFO:root:\t Processed 640 rows ...\n",
      "INFO:root:\t Processed 650 rows ...\n",
      "INFO:root:\t Processed 660 rows ...\n",
      "INFO:root:\t Processed 670 rows ...\n",
      "INFO:root:\t Processed 680 rows ...\n",
      "INFO:root:\t Processed 690 rows ...\n",
      "INFO:root:\t Processed 700 rows ...\n",
      "INFO:root:\t Processed 710 rows ...\n",
      "INFO:root:\t Processed 720 rows ...\n",
      "INFO:root:\t Processed 730 rows ...\n",
      "INFO:root:\t Processed 740 rows ...\n",
      "INFO:root:\t Processed 750 rows ...\n",
      "INFO:root:\t Processed 760 rows ...\n",
      "INFO:root:\t Processed 770 rows ...\n",
      "INFO:root:\t Processed 780 rows ...\n",
      "INFO:root:\t Processed 790 rows ...\n",
      "INFO:root:\t Processed 800 rows ...\n",
      "INFO:root:\t Processed 810 rows ...\n",
      "INFO:root:\t Processed 820 rows ...\n",
      "INFO:root:\t Processed 830 rows ...\n",
      "INFO:root:\t Processed 840 rows ...\n",
      "INFO:root:\t Processed 850 rows ...\n",
      "INFO:root:\t Processed 860 rows ...\n",
      "INFO:root:\t Processed 870 rows ...\n",
      "INFO:root:\t Processed 880 rows ...\n",
      "INFO:root:\t Processed 890 rows ...\n",
      "INFO:root:\t Processed 900 rows ...\n",
      "INFO:root:\t Processed 910 rows ...\n",
      "INFO:root:\t Processed 920 rows ...\n",
      "INFO:root:\t Processed 930 rows ...\n",
      "INFO:root:\t Processed 940 rows ...\n",
      "INFO:root:\t Processed 950 rows ...\n",
      "INFO:root:\t Processed 960 rows ...\n",
      "INFO:root:\t Processed 970 rows ...\n",
      "INFO:root:\t Processed 980 rows ...\n",
      "INFO:root:\t Processed 990 rows ...\n",
      "INFO:root:\t Processed 1000 rows ...\n",
      "INFO:root:\t Processed 1010 rows ...\n",
      "INFO:root:\t Processed 1020 rows ...\n",
      "INFO:root:\t Processed 1030 rows ...\n",
      "INFO:root:\t Processed 1040 rows ...\n",
      "INFO:root:\t Processed 1050 rows ...\n",
      "INFO:root:\t Processed 1060 rows ...\n",
      "INFO:root:\t Processed 1070 rows ...\n",
      "INFO:root:\t Processed 1080 rows ...\n",
      "INFO:root:\t Processed 1090 rows ...\n",
      "INFO:root:\t Processed 1100 rows ...\n",
      "INFO:root:\t Processed 1110 rows ...\n",
      "INFO:root:\t Processed 1120 rows ...\n",
      "INFO:root:\t Processed 1130 rows ...\n",
      "INFO:root:\t Processed 1140 rows ...\n",
      "INFO:root:\t Processed 1150 rows ...\n",
      "INFO:root:\t Processed 1160 rows ...\n",
      "INFO:root:\t Processed 1170 rows ...\n",
      "INFO:root:\t Processed 1180 rows ...\n",
      "INFO:root:\t Processed 1190 rows ...\n",
      "INFO:root:\t Processed 1200 rows ...\n",
      "INFO:root:\t Processed 1210 rows ...\n",
      "INFO:root:\t Processed 1220 rows ...\n",
      "INFO:root:\t Processed 1230 rows ...\n",
      "INFO:root:\t Processed 1240 rows ...\n",
      "INFO:root:\t Processed 1250 rows ...\n",
      "INFO:root:\t Processed 1260 rows ...\n",
      "INFO:root:\t Processed 1270 rows ...\n",
      "INFO:root:\t Processed 1280 rows ...\n",
      "INFO:root:\t Processed 1290 rows ...\n",
      "INFO:root:\t Processed 1300 rows ...\n",
      "INFO:root:\t Processed 1310 rows ...\n",
      "INFO:root:\t Processed 1320 rows ...\n",
      "INFO:root:\t Processed 1330 rows ...\n",
      "INFO:root:\t Processed 1340 rows ...\n",
      "INFO:root:\t Processed 1350 rows ...\n",
      "INFO:root:\t Processed 1360 rows ...\n",
      "INFO:root:\t Processed 1370 rows ...\n",
      "INFO:root:\t Processed 1380 rows ...\n",
      "INFO:root:\t Processed 1390 rows ...\n",
      "INFO:root:\t Processed 1400 rows ...\n",
      "INFO:root:\t Processed 1410 rows ...\n",
      "INFO:root:\t Processed 1420 rows ...\n",
      "INFO:root:\t Processed 1430 rows ...\n",
      "INFO:root:\t Processed 1440 rows ...\n",
      "INFO:root:\t Processed 1450 rows ...\n",
      "INFO:root:\t Processed 1460 rows ...\n",
      "INFO:root:\t Processed 1470 rows ...\n",
      "INFO:root:\t Processed 1480 rows ...\n",
      "INFO:root:\t Processed 1490 rows ...\n",
      "INFO:root:\t Processed 1500 rows ...\n",
      "INFO:root:\t Processed 1510 rows ...\n",
      "INFO:root:\t Processed 1520 rows ...\n",
      "INFO:root:\t Processed 1530 rows ...\n",
      "INFO:root:\t Processed 1540 rows ...\n",
      "INFO:root:\t Processed 1550 rows ...\n",
      "INFO:root:\t Processed 1560 rows ...\n",
      "INFO:root:\t Processed 1570 rows ...\n",
      "INFO:root:\t Processed 1580 rows ...\n",
      "INFO:root:\t Processed 1590 rows ...\n",
      "INFO:root:\t Processed 1600 rows ...\n",
      "INFO:root:\t Processed 1610 rows ...\n",
      "INFO:root:\t Processed 1620 rows ...\n",
      "INFO:root:\t Processed 1630 rows ...\n",
      "INFO:root:\t Processed 1640 rows ...\n",
      "INFO:root:\t Processed 1650 rows ...\n",
      "INFO:root:\t Processed 1660 rows ...\n",
      "INFO:root:\t Processed 1670 rows ...\n",
      "INFO:root:\t Processed 1680 rows ...\n",
      "INFO:root:\t Processed 1690 rows ...\n",
      "INFO:root:\t Processed 1700 rows ...\n",
      "INFO:root:\t Processed 1710 rows ...\n",
      "INFO:root:\t Processed 1720 rows ...\n",
      "INFO:root:\t Processed 1730 rows ...\n",
      "INFO:root:\t Processed 1740 rows ...\n",
      "INFO:root:\t Processed 1750 rows ...\n",
      "INFO:root:\t Processed 1760 rows ...\n",
      "INFO:root:\t Processed 1770 rows ...\n",
      "INFO:root:\t Processed 1780 rows ...\n",
      "INFO:root:\t Processed 1790 rows ...\n",
      "INFO:root:\t Processed 1800 rows ...\n",
      "INFO:root:\t Processed 1810 rows ...\n",
      "INFO:root:\t Processed 1820 rows ...\n",
      "INFO:root:\t Processed 1830 rows ...\n",
      "INFO:root:\t Processed 1840 rows ...\n",
      "INFO:root:\t Processed 1850 rows ...\n",
      "INFO:root:\t Processed 1860 rows ...\n",
      "INFO:root:\t Processed 1870 rows ...\n",
      "INFO:root:\t Processed 1880 rows ...\n",
      "INFO:root:\t Processed 1890 rows ...\n",
      "INFO:root:\t Processed 1900 rows ...\n",
      "INFO:root:\t Processed 1910 rows ...\n",
      "INFO:root:\t Processed 1920 rows ...\n",
      "INFO:root:\t Processed 1930 rows ...\n",
      "INFO:root:\t Processed 1940 rows ...\n",
      "INFO:root:\t Processed 1950 rows ...\n",
      "INFO:root:\t Processed 1960 rows ...\n",
      "INFO:root:Done!\n",
      "INFO:root:\t Inserted: 1970 total rows\n",
      "INFO:root:\t Failed to insert: 0 total rows\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "insert_data(TABLE, inserts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the whole process end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE WHOLE THING!\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# 2. ETL BQ Schema to Astra Schema\n",
    "bq_schema = get_bq_schema(\"aqueduct-production\", \"ds_metrics\", \"dse_new_customers_daily\")\n",
    "astra_schema = transform_bq_schema(\"dse_new_customers_daily\", bq_schema)\n",
    "\n",
    "# 3. Create Table in Astra\n",
    "token = get_token()\n",
    "result = create_table(astra_schema[\"dse_new_customers_daily\"])\n",
    "\n",
    "# 4. ETL Data from BQ to Astra payloads\n",
    "data = get_data_from_BQ(\"aqueduct-production\", \"ds_metrics\", \"dse_new_customers_daily\")\n",
    "inserts = transform_data(\"dse_new_customers_daily\", list(data.columns), data)\n",
    "\n",
    "# 5. Post payloads to Astra\n",
    "insert_data(\"dse_new_customers_daily\", inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DONE! Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
